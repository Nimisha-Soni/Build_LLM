# Build_LLM

![LLM Image](https://github.com/Nimisha-Soni/Build_LLM/blob/main/LLM.png)

## Tokenisation
1.)Tokenization in building large language models (LLMs) refers to the process of breaking down raw text into smaller units called tokens, which are the foundational elements the model uses to process and generate language.
2.)The choice of tokenization directly impacts efficiency, memory use, vocabulary coverage, and the ability of LLMs to deal with new words or complex text.
