{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e4870d5-fa5f-4dd6-844d-36568a81f8ac",
   "metadata": {},
   "source": [
    "Creating Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dc934ff-d4cb-4c9d-9fa8-d267f0706066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters : 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\",\"r\",encoding = \"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Total number of characters :\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83085b74-bcf5-43fd-ba2e-73da333090b6",
   "metadata": {},
   "source": [
    "We'll use a python library named \"Regular Expression\" to split the text on the basis of white spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "356aeea3-b064-494c-b8cc-91307b3890b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'There.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Hello, There. This, is a test\"\n",
    "result = re.split(r'(\\s)',text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32a5ffc4-1f75-46aa-9e37-84604cb3fe53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'There', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)',text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b28b50b7-4e71-4661-b0b4-a8f2eff31116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'There', '.', 'This', ',', 'is', 'a', 'test']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c44d63a4-49ed-40ef-9ecb-611a3f828719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "# now preprocessing the dataset\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)',raw_text)\n",
    "preprocessed = [item for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b8d3ece-e1a8-4689-9fc5-ff0efb42a830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e091539d-bc39-49fc-9c52-c837f0696c5f",
   "metadata": {},
   "source": [
    "Creating token ID's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ba8e67a-a76d-41dd-bed7-e54bb2bc19eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c3c3e90d-4d0a-4004-959f-3e933d01d2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need to assign token id to each word of vocab also called as encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2b195781-e9d5-4928-a937-ae74921407dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token : integer for integer,token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "54c3e89d-c34f-48bb-bebb-a38948ae6e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n",
      "('His', 51)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i>50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e6548fd7-174e-4ab8-8aef-69255bbde3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self,vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;\"?()\\']|--|\\s)',text)\n",
    "        preprocessed = [item for item in preprocessed if item.strip()]\n",
    "        ids = [ self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        # here we join the tokens individually\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # replace spaces before the puntuations\n",
    "        text = re.sub(r'\\s+([,.!\"()\\'])', r'\\1',text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9dffadc0-4992-4c6b-a2c6-591413ef2800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "# instantiate new tokenizer object from the SimpleTokenizerV1 class\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"It's the last he painted, you know,\"Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c16b575a-3f6c-4412-944c-1b4ad9234a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59452845-659c-4a46-b09b-49d3aeb969a7",
   "metadata": {},
   "source": [
    "Above we have implemented tokenisation and we have seen that tokenizer with the help of encode and decode method is capable of applying it on the text based ongiven statement.\n",
    "Now lets see if it can do it on the sample text that is not contained in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5a2ffd3e-2722-4638-80a5-65df26ce9ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello, do you like tea?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5e824c47-6898-45a7-b880-f62fd45c3d19",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mencode(text))\n",
      "Cell \u001b[1;32mIn[50], line 9\u001b[0m, in \u001b[0;36mSimpleTokenizerV1.encode\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m      7\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.:;\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m,text)\n\u001b[0;32m      8\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [item \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[1;32m----> 9\u001b[0m ids \u001b[38;5;241m=\u001b[39m [ \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_to_int[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Hello'"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "97795f5c-f52b-4d67-bf78-ec877f9c5f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see that due to limited tokens dataset here comes the error. Hence to train the model for LLM's we use large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081f8c0a-7b8e-47e5-a0ed-b8ea6da39f25",
   "metadata": {},
   "source": [
    "Adding Special Context Tokens \n",
    "In this section we will modify the tokens to handle unknown words\n",
    "Particularly we will modify the vocabulary and tokenizer we implemented in previous section, SimpleTokenizerV2 to support new tokens,<|unk|> and <|endoftext|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "547a277b-5210-4fdf-9558-536ad494485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(set(preprocessed))\n",
    "all_tokens.extend([\"<|endoftext|>\",\"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d37e64b4-f988-4709-a0e3-cd19d662e935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f58aeafb-80f3-41e8-8404-2c5a2050a621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# last vocab has 1130 length and new vocab has 2 more tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "208a0b4d-3cce-4689-8594-d88c337132b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "for i,item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "63d77d6f-c030-4dbd-8396-b27a58b461d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self,vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self,text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.str_to_int \n",
    "                       else \"<|unk|>\" for item in preprocessed]\n",
    "        \n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self,ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # replace spaces before punchuations\n",
    "        text = re.sub(r'\\s+([,.;:?!\"()\\'])',r'\\1',text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f3653fb8-2c85-415e-bc5e-18a0994f0c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "a0b50478-fb00-4d10-b70e-6f160d464a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "cf0d3168-1692-4e56-a48a-90896e108741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16413687-45be-4f37-a75f-ee5a5d5cbdc7",
   "metadata": {},
   "source": [
    "The tokenizer used by GPT does not use an '<|unk|>' token for out of vocabulary words. Instead it uses byte pair encoding tokenizer, which breaks down the words into subwords units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b467df9c-0fcc-418b-a289-6dc8b3d6d69f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
